{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c50da77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dahong/anaconda3/envs/coauthor/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from index import extract_paper_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f35ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(path=\"./contents/qdrant_storage\")\n",
    "if client.collection_exists(\"llama2_bm42\"):\n",
    "    client.delete_collection(\"llama2_bm42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b97392ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Techcrunch articles: 129\n",
      "Doc ID: dde26d00-fc1f-4e7e-b251-8a6a50b56a95\n",
      "Text: Published as a conference paper at ICLR 2021 AN IMAGE IS WORTH\n",
      "16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE Alexey\n",
      "Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk\n",
      "Weissenborn∗, Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani,\n",
      "Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil\n",
      "Houlsby∗,† ∗equal tech...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:01<00:00,  5.51it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"./contents/files/\",\n",
    "    file_metadata=extract_paper_metadata,\n",
    ")\n",
    "\n",
    "docs = reader.load_data()\n",
    "print(f\"Count of Techcrunch articles: {len(docs)}\")\n",
    "print(docs[0])\n",
    "\n",
    "client = QdrantClient(path=\"./contents/qdrant_storage\")\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=\"llama2_bm42\",\n",
    "    client=client,\n",
    "    fastembed_sparse_model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1bcba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dahong/anaconda3/envs/coauthor/lib/python3.11/site-packages/llama_index/vector_stores/qdrant/base.py:703: UserWarning: Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "  self._client.create_payload_index(\n"
     ]
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    # our dense embedding model\n",
    "    embed_model=embed_model,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e4e6c",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632933d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(path=\"./contents/qdrant_storage\")\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=\"llama2_bm42\",\n",
    "    client=client,\n",
    "    fastembed_sparse_model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    ")\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    ")\n",
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538216bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = retriever.retrieve(\"What is Audio spectrogram transformer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d98d18a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Audio Spectrogram Transformer\n",
      "2.1. Model Architecture\n",
      "Figure 1 illustrates the proposed Audio Spectrogram Trans-\n",
      "former (AST) architecture. First, the input audio waveform of t\n",
      "seconds is converted into a sequence of 128-dimensional log\n",
      "Mel ﬁlterbank (fbank) features computed with a 25ms Ham-\n",
      "ming window every 10ms. This results in a128 ×100t spectro-\n",
      "gram as input to the AST. We then split the spectrogram into a\n",
      "sequence of N 16×16 patches with an overlap of 6 in both time\n",
      "and frequency dimension, where N = 12⌈(100t −16)/10⌉is\n",
      "the number of patches and the effective input sequence length\n",
      "for the Transformer. We ﬂatten each16×16 patch to a 1D patch\n",
      "embedding of size 768 using a linear projection layer. We re-\n",
      "fer to this linear projection layer as the patch embedding layer.\n",
      "Since the Transformer architecture does not capture the input\n",
      "order information and the patch sequence is also not in tem-\n",
      "poral order, we add a trainable positional embedding (also of\n",
      "size 768) to each patch embedding to allow the model to cap-\n",
      "ture the spatial structure of the 2D audio spectrogram.\n",
      "Similar to [22], we append a [CLS] token at the beginning\n",
      "of the sequence. The resulting sequence is then input to the\n",
      "Transformer. A Transformer consists of several encoder and\n",
      "decoder layers. Since AST is designed for classiﬁcation tasks,\n",
      "we only use the encoder of the Transformer. Intentionally, we\n",
      "use the original Transformer encoder [18] architecture without\n",
      "modiﬁcation. The advantages of this simple setup are 1) the\n",
      "standard Transformer architecture is easy to implement and re-\n",
      "produce as it is off-the-shelf in TensorFlow and PyTorch, and\n",
      "2) we intend to apply transfer learning for AST, and a stan-\n",
      "dard architecture makes transfer learning easier. Speciﬁcally,\n",
      "the Transformer encoder we use has an embedding dimension\n",
      "of 768, 12 layers, and 12 heads, which are the same as those\n",
      "in [12, 11]. The Transformer encoder’s output of the [CLS]\n",
      "token serves as the audio spectrogram representation. A linear\n",
      "layer with sigmoid activation maps the audio spectrogram rep-\n",
      "resentation to labels for classiﬁcation.\n",
      "Strictly speaking, the patch embedding layer can be viewed\n",
      "as a single convolution layer with a large kernel and stride size,\n",
      "and the projection layer in each Transformer block is equivalent\n",
      "to 1×1 convolution. However, the design is different from con-\n",
      "ventional CNNs that have multiple layers and small kernel and\n",
      "stride sizes. These Transformer models are usually referred to\n",
      "as convolution-free to distinguish them from CNNs [11, 12].\n",
      "2.2. ImageNet Pretraining\n",
      "One disadvantage of the Transformer compared with CNNs is\n",
      "that the Transformer needs more data to train [11]. In [11],\n",
      "the authors point out that the Transformer only starts to out-\n",
      "perform CNNs when the amount of data is over 14 million for\n",
      "image classiﬁcation tasks. However, audio datasets typically\n",
      "do not have such large amounts of data, which motivates us\n",
      "to apply cross-modality transfer learning to AST since images\n",
      "and audio spectrograms have similar formats. Transfer learn-\n",
      "ing from vision tasks to audio tasks has been previously stud-\n",
      "ied in [23, 24, 25, 8], but only for CNN-based models, where\n",
      "ImageNet-pretrained CNN weights are used as initial CNN\n",
      "weights for audio classiﬁcation training. In practice, it is com-\n",
      "putationally expensive to train a state-of-the-art vision model,\n",
      "but many commonly used architectures (e.g., ResNet [26], Ef-\n",
      "ﬁcientNet [27]) have off-the-shelf ImageNet-pretrained mod-\n",
      "els for both TensorFlow and PyTorch, making transfer learning\n",
      "much easier. We also follow this regime by adapting an off-the-\n",
      "shelf pretrained Vision Transformer (ViT) to AST.\n",
      "While ViT and AST have similar architectures (e.g., both\n",
      "use a standard Transformer, same patch size, same embedding\n",
      "size), they are not same. Therefore, a few modiﬁcations need to\n",
      "make for the adaptation.\n",
      "\n",
      "\n",
      "\n",
      "Second, AST nat-\n",
      "urally supports variable-length inputs and can be applied to dif-\n",
      "ferent tasks without any change of architecture. Speciﬁcally, the\n",
      "Code at https://github.com/YuanGongND/ast.\n",
      "Transformer Encoder\n",
      "Linear ProjectionE[CLS]\n",
      "P[0]\n",
      "E[1]E[2]E[3]E[4]E[5]E[6]E[7]E[8]\n",
      "P[1]P[2]P[3]P[4]P[5]P[6]P[7]P[8]\n",
      "LinearOutput\n",
      "Patch Split with Overlap\n",
      "PatchEmbedding\n",
      "Positional Embedding\n",
      "Patch Embedding\n",
      "Input Spectrogram\n",
      "1 2 3 456 78\n",
      "1 2 3 4 5 6 7 8\n",
      "Figure 1: The proposed audio spectrogram transformer (AST)\n",
      "architecture. The 2D audio spectrogram is split into a sequence\n",
      "of 16×16 patches with overlap, and then linearly projected to\n",
      "a sequence of 1-D patch embeddings. Each patch embedding\n",
      "is added with a learnable positional embedding. An additional\n",
      "classiﬁcation token is prepended to the sequence. The output\n",
      "embedding is input to a Transformer, and the output of the clas-\n",
      "siﬁcation token is used for classiﬁcation with a linear layer.\n",
      "models we use for all aforementioned tasks have the same archi-\n",
      "tecture while the input lengths vary from 1 sec. (Speech Com-\n",
      "mands) to 10 sec. (AudioSet). In contrast, CNN-based models\n",
      "typically require architecture tuning to obtain optimal perfor-\n",
      "mance for different tasks. Third, comparing with SOTA CNN-\n",
      "attention hybrid models, AST features a simpler architecture\n",
      "with fewer parameters, and converges faster during training. To\n",
      "the best of our knowledge, AST is the ﬁrst purely attention-\n",
      "based audio classiﬁcation model.\n",
      "Related Work The proposed Audio Spectrogram Trans-\n",
      "former, as the name suggests, is based on the Transformer ar-\n",
      "chitecture [18], which was originally proposed for natural lan-\n",
      "guage processing tasks. Recently, the Transformer has also\n",
      "been adapted for audio processing, but is typically used in\n",
      "conjunction with a CNN [19, 20, 21]. In [19, 20], the au-\n",
      "thors stack a Transformer on top of a CNN, while in [21],\n",
      "the authors combine a Transformer and a CNN in each model\n",
      "block. Other efforts combine CNNs with simpler attention\n",
      "modules [8, 7, 9]. The proposed AST differs from these stud-\n",
      "ies in that it is convolution-free and purely based on attention\n",
      "mechanisms. The closest work to ours is the Vision Trans-\n",
      "former (ViT) [11, 12, 13], which is a Transformer architecture\n",
      "for vision tasks. AST and ViT have similar architectures but\n",
      "ViT has only been applied to ﬁxed-dimensional inputs (images)\n",
      "while AST can process variable-length audio inputs. In addi-\n",
      "tion, we propose an approach to transfer knowledge from Ima-\n",
      "geNet pretrained ViT to AST. We also conduct extensive exper-\n",
      "iments to show the design choice of AST on audio tasks.\n",
      "arXiv:2104.01778v3  [cs.SD]  8 Jul 2021\n",
      "\n",
      "\n",
      "\n",
      "AST: Audio Spectrogram Transformer\n",
      "Yuan Gong, Yu-An Chung, James Glass\n",
      "MIT Computer Science and Artiﬁcial Intelligence Laboratory, Cambridge, MA 02139, USA\n",
      "{yuangong, andyyuan, glass}@mit.edu\n",
      "Abstract\n",
      "In the past decade, convolutional neural networks (CNNs)\n",
      "have been widely adopted as the main building block for end-\n",
      "to-end audio classiﬁcation models, which aim to learn a direct\n",
      "mapping from audio spectrograms to corresponding labels. To\n",
      "better capture long-range global context, a recent trend is to\n",
      "add a self-attention mechanism on top of the CNN, forming a\n",
      "CNN-attention hybrid model. However, it is unclear whether\n",
      "the reliance on a CNN is necessary, and if neural networks\n",
      "purely based on attention are sufﬁcient to obtain good perfor-\n",
      "mance in audio classiﬁcation. In this paper, we answer the ques-\n",
      "tion by introducing the Audio Spectrogram Transformer(AST),\n",
      "the ﬁrst convolution-free, purely attention-based model for au-\n",
      "dio classiﬁcation. We evaluate AST on various audio classiﬁ-\n",
      "cation benchmarks, where it achieves new state-of-the-art re-\n",
      "sults of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50,\n",
      "and 98.1% accuracy on Speech Commands V2.\n",
      "Index Terms: audio classiﬁcation, self-attention, Transformer\n",
      "1. Introduction\n",
      "With the advent of deep neural networks, over the last decade\n",
      "audio classiﬁcation research has moved from models based\n",
      "on hand-crafted features [1, 2] to end-to-end models that di-\n",
      "rectly map audio spectrograms to corresponding labels [3, 4, 5].\n",
      "Speciﬁcally, convolutional neural networks (CNNs) [6] have\n",
      "been widely used to learn representations from raw spectro-\n",
      "grams for end-to-end modeling, as the inductive biases inherent\n",
      "to CNNs such as spatial locality and translation equivariance\n",
      "are believed to be helpful. In order to better capture long-range\n",
      "global context, a recent trend is to add a self-attention mech-\n",
      "anism on top of the CNN. Such CNN-attention hybrid mod-\n",
      "els have achieved state-of-the-art (SOTA) results for many au-\n",
      "dio classiﬁcation tasks such as audio event classiﬁcation [7, 8],\n",
      "speech command recognition [9], and emotion recognition [10].\n",
      "However, motivated by the success of purely attention-based\n",
      "models in the vision domain [11, 12, 13], it is reasonable to ask\n",
      "whether a CNN is still essential for audio classiﬁcation.\n",
      "To answer the question, we introduce the Audio Spectro-\n",
      "gram Transformer (AST), a convolution-free, purely attention-\n",
      "based model that is directly applied to an audio spectrogram\n",
      "and can capture long-range global context even in the lowest\n",
      "layers. Additionally, we propose an approach for transferring\n",
      "knowledge from the Vision Transformer (ViT) [12] pretrained\n",
      "on ImageNet [14] to AST, which can signiﬁcantly improve the\n",
      "performance. The advantages of AST are threefold. First, AST\n",
      "has superior performance: we evaluate AST on a variety of au-\n",
      "dio classiﬁcation tasks and datasets including AudioSet [15],\n",
      "ESC-50 [16] and Speech Commands [17]. AST outperforms\n",
      "state-of-the-art systems on all these datasets. Second, AST nat-\n",
      "urally supports variable-length inputs and can be applied to dif-\n",
      "ferent tasks without any change of architecture. Speciﬁcally, the\n",
      "Code at https://github.com/YuanGongND/ast.\n",
      "Transformer Encoder\n",
      "Linear ProjectionE[CLS]\n",
      "P[0]\n",
      "E[1]E[2]E[3]E[4]E[5]E[6]E[7]E[8]\n",
      "P[1]P[2]P[3]P[4]P[5]P[6]P[7]P[8]\n",
      "LinearOutput\n",
      "Patch Split with Overlap\n",
      "PatchEmbedding\n",
      "Positional Embedding\n",
      "Patch Embedding\n",
      "Input Spectrogram\n",
      "1 2 3 456 78\n",
      "1 2 3 4 5 6 7 8\n",
      "Figure 1: The proposed audio spectrogram transformer (AST)\n",
      "architecture.\n",
      "\n",
      "\n",
      "\n",
      "We present\n",
      "our primary AudioSet results and ablation study in Section 3.1.2\n",
      "and Section 3.1.3, respectively. We then present our experi-\n",
      "ments on ESC-50 and Speech Commands V2 in Section 3.2.\n",
      "3.1. AudioSet Experiments\n",
      "3.1.1. Dataset and Training Details\n",
      "AudioSet [15] is a collection of over 2 million 10-second au-\n",
      "dio clips excised from YouTube videos and labeled with the\n",
      "sounds that the clip contains from a set of 527 labels. The bal-\n",
      "anced training, full training, and evaluation set contains 22k,\n",
      "2M, and 20k samples, respectively. For AudioSet experiments,\n",
      "we use the exact same training pipeline with [8]. Speciﬁcally,\n",
      "we use ImageNet pretraining (as described in Section 2.2), bal-\n",
      "anced sampling (for full set experiments only), data augmenta-\n",
      "\n",
      "\n",
      "\n",
      "In practice, it is com-\n",
      "putationally expensive to train a state-of-the-art vision model,\n",
      "but many commonly used architectures (e.g., ResNet [26], Ef-\n",
      "ﬁcientNet [27]) have off-the-shelf ImageNet-pretrained mod-\n",
      "els for both TensorFlow and PyTorch, making transfer learning\n",
      "much easier. We also follow this regime by adapting an off-the-\n",
      "shelf pretrained Vision Transformer (ViT) to AST.\n",
      "While ViT and AST have similar architectures (e.g., both\n",
      "use a standard Transformer, same patch size, same embedding\n",
      "size), they are not same. Therefore, a few modiﬁcations need to\n",
      "make for the adaptation. First, the input of ViT is a 3-channel\n",
      "image while the input to the AST is a single-channel spectro-\n",
      "gram, we average the weights corresponding to each of the\n",
      "three input channels of the ViT patch embedding layer and use\n",
      "them as the weights of the AST patch embedding layer. This\n",
      "is equivalent to expanding a single-channel spectrogram to 3-\n",
      "channels with the same content, but is computationally more\n",
      "efﬁcient. We also normalize the input audio spectrogram so that\n",
      "the dataset mean and standard deviation are 0 and 0.5, respec-\n",
      "tively. Second, the input shape of ViT is ﬁxed (either224 ×224\n",
      "or 384 ×384), which is different from a typical audio spectro-\n",
      "gram. In addition, the length of an audio spectrogram can be\n",
      "variable. While the Transformer naturally supports variable in-\n",
      "put length and can be directly transferred from ViT to AST, the\n",
      "positional embedding needs to be carefully processed because\n",
      "it learns to encode the spatial information during the ImageNet\n",
      "training. We propose a cut and bi-linear interpolate method for\n",
      "positional embedding adaptation. For example, for a ViT that\n",
      "takes 384 ×384 image input and uses a patch size of 16 ×16,\n",
      "the number of patches and corresponding positional embedding\n",
      "is 24 ×24 = 576 (ViT splits patches without overlap). An\n",
      "AST that takes 10-second audio input has 12 ×100 patches,\n",
      "each patch needs a positional embedding. We therefore cut\n",
      "the ﬁrst dimension and interpolate the second dimension of the\n",
      "24 ×24 ViT positional embedding to 12 ×100 and use it as\n",
      "the positional embedding for the AST. We directly reuse the\n",
      "positional embedding for the [CLS] token. By doing this we\n",
      "are able to transfer the 2D spatial knowledge from a pretrained\n",
      "ViT to the AST even when the input shapes are different. Fi-\n",
      "nally, since the classiﬁcation task is essentially different, we\n",
      "abandon the last classiﬁcation layer of the ViT and reinitialize\n",
      "a new one for AST. With this adaptation framework, the AST\n",
      "can use various pretrained ViT weights for initialization. In this\n",
      "work, we use pretrained weights of a data-efﬁcient image Trans-\n",
      "former (DeiT) [12], which is trained with CNN knowledge dis-\n",
      "tillation, 384 ×384 images, has 87M parameters, and achieves\n",
      "85.2% top-1 accuracy on ImageNet 2012. During ImageNet\n",
      "training, DeiT has two [CLS] tokens; we average them as a\n",
      "single [CLS] token for audio training.\n",
      "3. Experiments\n",
      "In this section, we focus on evaluating the AST on AudioSet\n",
      "(Section 3.1) as weakly-labeled audio event classiﬁcation is one\n",
      "of the most challenging audio classiﬁcation tasks. We present\n",
      "our primary AudioSet results and ablation study in Section 3.1.2\n",
      "and Section 3.1.3, respectively. We then present our experi-\n",
      "ments on ESC-50 and Speech Commands V2 in Section 3.2.\n",
      "3.1. AudioSet Experiments\n",
      "3.1.1. Dataset and Training Details\n",
      "AudioSet [15] is a collection of over 2 million 10-second au-\n",
      "dio clips excised from YouTube videos and labeled with the\n",
      "sounds that the clip contains from a set of 527 labels. The bal-\n",
      "anced training, full training, and evaluation set contains 22k,\n",
      "2M, and 20k samples, respectively. For AudioSet experiments,\n",
      "we use the exact same training pipeline with [8].\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    print(node.get_content())\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a393d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coauthor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
