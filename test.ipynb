{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c50da77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274e7d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Techcrunch articles: 129\n",
      "Doc ID: c95c9023-9715-4363-a6b7-92b825ed53b2\n",
      "Text: Published as a conference paper at ICLR 2021 AN IMAGE IS WORTH\n",
      "16X16 W ORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE Alexey\n",
      "Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk\n",
      "Weissenborn∗, Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani,\n",
      "Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil\n",
      "Houlsby∗,† ∗equal tech...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dahong/anaconda3/envs/coauthor/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating embeddings: 100%|██████████| 450/450 [00:07<00:00, 59.15it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"../contents/files/\",\n",
    ")\n",
    "\n",
    "docs = reader.load_data()\n",
    "print(f\"Count of Techcrunch articles: {len(docs)}\")\n",
    "print(docs[0])\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=100)\n",
    "nodes = node_parser.get_nodes_from_documents(docs)\n",
    "index = VectorStoreIndex(nodes, embed_model=embed_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c1f186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 98bfea55-fc22-428e-baf2-69bdfe8759f5\n",
      "Text: T2T-ViT Backbone As many channels in the backbone of vanilla ViT\n",
      "are in- valid (Fig. 2), we plan to ﬁnd an efﬁcient backbone for our\n",
      "T2T-ViT to reduce the redundancy and improve the feature richness.\n",
      "Thus we explore different architecture designs for ViT and borrow some\n",
      "designs from CNNs to improve the backbone efﬁciency and enhance the\n",
      "richness...\n",
      "Score:  0.407\n",
      "\n",
      "T2T-ViT Backbone\n",
      "As many channels in the backbone of vanilla ViT are in-\n",
      "valid (Fig. 2), we plan to ﬁnd an efﬁcient backbone for our\n",
      "T2T-ViT to reduce the redundancy and improve the feature\n",
      "richness. Thus we explore different architecture designs for\n",
      "ViT and borrow some designs from CNNs to improve the\n",
      "backbone efﬁciency and enhance the richness of the learned\n",
      "features. As each transformer layer has skip connection as\n",
      "ResNets, a straightforward idea is to apply dense connec-\n",
      "tion as DenseNet [21] to increase the connectivity and fea-\n",
      "ture richness, or apply Wide-ResNets or ResNeXt structure\n",
      "to change the channel dimension and head number in the\n",
      "backbone of ViT. We explore ﬁve architecture designs from\n",
      "CNNs to ViT:\n",
      "1. Dense connection as DenseNet [21];\n",
      "2. Deep-narrow vs. shallow-wide structure as in Wide-\n",
      "ResNets [52];\n",
      "3. Channel attention as Squeeze-an-Excitation (SE) Net-\n",
      "works [20];\n",
      "4. More split heads in multi-head attention layer as\n",
      "ResNeXt [44];\n",
      "5. Ghost operations as GhostNet [14].\n",
      "The details of these structure designs in ViT are given in the\n",
      "appendix. We conduct extensive experiments on the struc-\n",
      "tures transferring in Sec. 4.2. We empirically ﬁnd that 1)\n",
      "by adopting a deep-narrow structure that simply decreases\n",
      "channel dimensions to reduce the redundancy in channels\n",
      "and increase layer depth to improve feature richness in ViT,\n",
      "both the model size and MACs are decreased but perfor-\n",
      "mance is improved; 2) the channel attention as SE block\n",
      "also improves ViT but is less effective than using the deep-\n",
      "narrow structure.\n",
      "Based on these ﬁndings, we design a deep-narrow ar-\n",
      "chitecture for our T2T-ViT backbone.\n",
      "===\n",
      "Node ID: 4596d5dc-8cc3-4177-bdaa-3d0230a091b4\n",
      "Text: For T2T transformer layer, we adopt Transformer layer for\n",
      "T2T-ViT t-14 and Performer layer for T2T-ViT-14 at limited GPU memory.\n",
      "For ViT, ‘S’ means Small, ‘B’ is Base and ‘L’ is Large. ‘ViT-S/16’ is\n",
      "a variant from original ViT-B/16 [12] with smaller MLP size and layer\n",
      "depth. Models Tokens-to-Token module T2T-ViT backbone Model size T2T\n",
      "transform...\n",
      "Score:  0.401\n",
      "\n",
      "For T2T transformer layer, we adopt Transformer layer for T2T-ViT t-14 and Performer layer\n",
      "for T2T-ViT-14 at limited GPU memory. For ViT, ‘S’ means Small, ‘B’ is Base and ‘L’ is Large. ‘ViT-S/16’ is a variant from original\n",
      "ViT-B/16 [12] with smaller MLP size and layer depth.\n",
      "Models\n",
      "Tokens-to-Token module T2T-ViT backbone Model size\n",
      "T2T\n",
      "transformer\n",
      "Depth Hidden\n",
      "dim\n",
      "MLP\n",
      "size\n",
      "Depth Hidden\n",
      "dim\n",
      "MLP\n",
      "size\n",
      "Params\n",
      "(M)\n",
      "MACs\n",
      "(G)\n",
      "ViT-S/16 [12] - - - - 8 786 2358 48.6 10.1\n",
      "ViT-B/16 [12] - - - - 12 786 3072 86.8 17.6\n",
      "ViT-L/16 [12] - - - - 24 1024 4096 304.3 63.6\n",
      "T2T-ViT-14 Performer 2 64 64 14 384 1152 21.5 4.8\n",
      "T2T-ViT-19 Performer 2 64 64 19 448 1344 39.2 8.5\n",
      "T2T-ViT-24 Performer 2 64 64 24 512 1536 64.1 13.8\n",
      "T2T-ViTt-14 Transformer 2 64 64 14 384 1152 21.5 6.1\n",
      "T2T-ViT-7 Performer 2 64 64 8 256 512 4.2 1.1\n",
      "T2T-ViT-12 Performer 2 64 64 12 256 512 6.8 1.8\n",
      "3.3. T2T-ViT Architecture\n",
      "The T2T-ViT has two parts: the Tokens-to-Token (T2T)\n",
      "module and the T2T-ViT backbone (Fig. 4).\n",
      "===\n",
      "Node ID: fc7a7db1-daf1-45f9-83a3-9d1ef6ebf9a1\n",
      "Text: We summarize the effects of each CNN-based structure below.\n",
      "Deep-narrow structure beneﬁts ViT: The models ViT- DN (Deep-Narrow)\n",
      "and ViT-SW (Shallow-Wide) in Tab. 6 are two opposite designs in\n",
      "channel dimension and layer depth, where ViT-DN has 384 hidden\n",
      "dimensions and 16 layers and ViT-SW has 1,024 hidden dimensions and 4\n",
      "lay- ers. Compared wit...\n",
      "Score:  0.401\n",
      "\n",
      "We\n",
      "summarize the effects of each CNN-based structure below.\n",
      "Deep-narrow structure beneﬁts ViT: The models ViT-\n",
      "DN (Deep-Narrow) and ViT-SW (Shallow-Wide) in Tab. 6\n",
      "are two opposite designs in channel dimension and layer\n",
      "depth, where ViT-DN has 384 hidden dimensions and 16\n",
      "layers and ViT-SW has 1,024 hidden dimensions and 4 lay-\n",
      "ers. Compared with the baseline model ViT-S/16 with 768\n",
      "hidden dimensions and 8 layers, shallow-wide model ViT-\n",
      "SW has 8.2% decrease in performance while ViT-DN with\n",
      "only half of model size and MACs achieve 0.9% increase.\n",
      "These results validate our hypothesis that vanilla ViT with\n",
      "shallow-wide structure is redundant in channel dimensions\n",
      "and limited feature richness with shallow layers.\n",
      "Dense connection hurts performance of both ViT and\n",
      "T2T-ViT: Compared with the ResNet50, DenseNet201\n",
      "has smaller parameters and comparable MACs, while it has\n",
      "higher performance. However, the dense connection can\n",
      "hurt performance of ViT-Dense and T2T-ViT-Dense (dark\n",
      "blue rows in Tab. 6).\n",
      "SE block improves both ViT and T2T-ViT: From red\n",
      "rows in Tab. 6, we can ﬁnd SENets, ViT-SE and T2T-ViT-\n",
      "SE are higher than the corresponding baseline. The SE\n",
      "module can improve performance on both CNN and ViT,\n",
      "which means applying attention to channels beneﬁts both\n",
      "CNN and ViT models.\n",
      "ResNeXt structure has few effects on ViT and T2T-ViT:\n",
      "ResNeXts adopt multi-head on ResNets, while Transform-\n",
      "ers are also multi-head attention structure. When we adopt\n",
      "more heads like 32, we can ﬁnd it has few effects on per-\n",
      "formance (red rows in Tab 6). However, adopting a large\n",
      "number of heads makes the GPU memory large, which is\n",
      "thus unnecessary in ViT and T2T-ViT.\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "query = \"Tell me moer about ViT\"\n",
    "\n",
    "for node in retriever.retrieve(query):\n",
    "    print(node)\n",
    "    print(node.get_content())\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6fda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(\n",
    "    persist_dir=\"../contents/indexes/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f082e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: d446eea8-0987-4890-9779-321db6fed418\n",
      "Text: 2. Audio Spectrogram Transformer 2.1. Model Architecture Figure\n",
      "1 illustrates the proposed Audio Spectrogram Trans- former (AST)\n",
      "architecture. First, the input audio waveform of t seconds is\n",
      "converted into a sequence of 128-dimensional log Mel ﬁlterbank (fbank)\n",
      "features computed with a 25ms Ham- ming window every 10ms. This\n",
      "results in a128 ×100t...\n",
      "Score:  0.041\n",
      "\n",
      "2. Audio Spectrogram Transformer\n",
      "2.1. Model Architecture\n",
      "Figure 1 illustrates the proposed Audio Spectrogram Trans-\n",
      "former (AST) architecture. First, the input audio waveform of t\n",
      "seconds is converted into a sequence of 128-dimensional log\n",
      "Mel ﬁlterbank (fbank) features computed with a 25ms Ham-\n",
      "ming window every 10ms. This results in a128 ×100t spectro-\n",
      "gram as input to the AST. We then split the spectrogram into a\n",
      "sequence of N 16×16 patches with an overlap of 6 in both time\n",
      "and frequency dimension, where N = 12⌈(100t −16)/10⌉is\n",
      "the number of patches and the effective input sequence length\n",
      "for the Transformer. We ﬂatten each16×16 patch to a 1D patch\n",
      "embedding of size 768 using a linear projection layer. We re-\n",
      "fer to this linear projection layer as the patch embedding layer.\n",
      "Since the Transformer architecture does not capture the input\n",
      "order information and the patch sequence is also not in tem-\n",
      "poral order, we add a trainable positional embedding (also of\n",
      "size 768) to each patch embedding to allow the model to cap-\n",
      "ture the spatial structure of the 2D audio spectrogram.\n",
      "Similar to [22], we append a [CLS] token at the beginning\n",
      "of the sequence. The resulting sequence is then input to the\n",
      "Transformer. A Transformer consists of several encoder and\n",
      "decoder layers. Since AST is designed for classiﬁcation tasks,\n",
      "we only use the encoder of the Transformer. Intentionally, we\n",
      "use the original Transformer encoder [18] architecture without\n",
      "modiﬁcation. The advantages of this simple setup are 1) the\n",
      "standard Transformer architecture is easy to implement and re-\n",
      "produce as it is off-the-shelf in TensorFlow and PyTorch, and\n",
      "2) we intend to apply transfer learning for AST, and a stan-\n",
      "dard architecture makes transfer learning easier. Speciﬁcally,\n",
      "the Transformer encoder we use has an embedding dimension\n",
      "of 768, 12 layers, and 12 heads, which are the same as those\n",
      "in [12, 11].\n",
      "===\n",
      "Node ID: 80f04d74-4eb8-48f5-a99b-f65e1cd0cd0e\n",
      "Text: We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3. In Table 3\n",
      "rows (A), we vary the number of attention heads and the attention key\n",
      "and value dimensions, keeping the amount of computation constant, as\n",
      "described in Section 3.2.2. While single-head attention is 0.9 BLEU\n",
      "worse...\n",
      "Score:  0.040\n",
      "\n",
      "We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\n",
      "results to the base model.\n",
      "6.3 English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "We trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9\n",
      "===\n",
      "Node ID: 0ff94110-7005-48e0-adec-5d2fcda82aaa\n",
      "Text: Provided proper attribution is provided, Google hereby grants\n",
      "permission to reproduce the tables and figures in this paper solely\n",
      "for use in journalistic or scholarly works. Attention Is All You Need\n",
      "Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google\n",
      "Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com\n",
      "Jakob Usz...\n",
      "Score:  0.038\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea.\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"../contents/indexes/\",\n",
    ")\n",
    "new_index = load_index_from_storage(storage_context)\n",
    "retriever = new_index.as_retriever(similarity_top_k=3)\n",
    "query = \"Tell me more about ViT\"\n",
    "for node in retriever.retrieve(query):\n",
    "    print(node)\n",
    "    print(node.get_content())\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97392ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coauthor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
